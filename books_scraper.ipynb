{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a625046-e11e-46c5-9e9d-0e83ec4917c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests import Response\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fec75313-c4c5-4659-a6d8-69ed4c12557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Scrapes the requirement elements from the page content provided\n",
    "def scrape_page(response: Response) -> None:\n",
    "\n",
    "    # Parse site's content html\n",
    "    content: bytes = response.content\n",
    "    soup: Beautifulsoup =  BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # Get the books from the list\n",
    "    ol = soup.find('ol')\n",
    "    articles = ol.find_all('article', class_='product_pod')\n",
    "    \n",
    "    data = []\n",
    "    # Extract the fields needed\n",
    "    for article in articles:\n",
    "        image = article.find('img')\n",
    "        title = image.attrs['alt']\n",
    "        star_elem = article.find('p')\n",
    "        star_num = star_elem.attrs['class'][1]\n",
    "        price = article.find('p', class_='price_color').get_text()\n",
    "        price_float = float(price[1:])\n",
    "    \n",
    "        data.append([title, price_float, star_num])\n",
    "        \n",
    "    return data\n",
    "        \n",
    "        \n",
    "    \n",
    "# Gets page and hanlde errors\n",
    "def get_page(url: str) -> Response:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"Error with requests for {url}: {e}\", \"Aborting...!\")\n",
    "        raise SystemExit(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f13f218c-cb14-45d2-a01e-64dc16fb1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "from requests.models import Response\n",
    "\n",
    "books: List[Any] = []\n",
    "# Get all pages and scrape them one by one while handling any errors\n",
    "for i in range(1, 51):\n",
    "    URL: str = f\"http://books.toscrape.com/catalogue/page-{i}.html\"\n",
    "    response = get_page(URL)\n",
    "\n",
    "    if response:\n",
    "        data = scrape_page(response)\n",
    "        books.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59637106-04a2-46c1-895d-0cfef1dc82ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(books, columns=['Title','Price', 'Star Rating'])\n",
    "df.to_csv('books.csv')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d89ba3-9550-47cb-b245-4027e1397002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data311",
   "language": "python",
   "name": "data311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
