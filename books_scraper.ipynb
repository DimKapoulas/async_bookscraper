{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16affb26-332a-46ed-9312-373a988bef21",
   "metadata": {},
   "source": [
    "# Multiple Book Scraping\n",
    "Here is a notebook demonstrating asynchronous scraping. \n",
    "The site used for this puprose is\n",
    "the **Books to Scape**, which you can finde [here](http://books.toscrape.com/index.html). Its sole purpose is that for scraping so you can hit with as many request you want without nothing to worry about.\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"assets/async_python_scraping_small.jpg\" width=\"auto\" height=\"auto\">\n",
    "</div>\n",
    "So let us get us going by importing our dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a625046-e11e-46c5-9e9d-0e83ec4917c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag\n",
    "import pandas as pd\n",
    "\n",
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "from typing import Any, List\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b598680-438b-4acc-979a-dfd6ea180758",
   "metadata": {},
   "source": [
    "### Function Definitions\n",
    "\n",
    "So besides our usual imports here\n",
    "we define two important functions:\n",
    "1. `scrape_page(content: bytes) -> List`: This function takes the content of a web page as input and extracts book-related data, such as title, star rating, and price, returning it as a list.\n",
    "\n",
    "2. `fetch_page(session: aiohttp.ClientSession, url: str) -> bytes`: This asynchronous function fetches the content of a web page using aiohttp. It sends an HTTP GET request to a given URL, handles any errors, and returns the page's content as bytes.\n",
    "\n",
    "3. `get_data(session: aiohttp.ClientSession, urls: List[str]) -> List`: This function is responsible for collecting data from multiple web pages asynchronously. This function takes a session and a list of URLs as input. In more detail:\n",
    "- It initializes an empty list called `all_data` to store the collected data.\n",
    "- It creates a list of asynchronous tasks (`tasks`) for fetching pages using the `fetch_page` function for each URL in the `urls` list.\n",
    "- It uses `asyncio.gather(*tasks)` to execute these tasks concurrently and gather the results (page content) into the `pages` list.\n",
    "- For each page content in the `pages` list, it calls the `scrape_page` function to extract book data and extends the `all_data` list with this data.\n",
    "- Finally, it returns the `all_data` list containing data from all the pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fec75313-c4c5-4659-a6d8-69ed4c12557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Scrapes the requirement elements from the page content provided\n",
    "def scrape_page(content: bytes) -> List:\n",
    "\n",
    "    # Parse site's content html\n",
    "    soup: Beautifulsoup =  BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # Get the books from the list\n",
    "    ol = soup.find('ol')\n",
    "    articles = ol.find_all('article', class_='product_pod')\n",
    "    data = []\n",
    "    \n",
    "    # Extract the fields needed\n",
    "    for article in articles:\n",
    "        image = article.find('img')\n",
    "        title = image.attrs['alt']\n",
    "        star_elem = article.find('p')\n",
    "        star_num = star_elem.attrs['class'][1]\n",
    "        price = article.find('p', class_='price_color').get_text()\n",
    "        price_float = float(price[1:])\n",
    "    \n",
    "        data.append([title, price_float, star_num])\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "# Fetches page, called asynchronously by \"parent\" function\n",
    "async def fetch_page(session: aiohttp.ClientSession, url: str) -> bytes:\n",
    "    headers = {'User-Agent': \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/117.0\",\n",
    "               \"X-Amzn-Trace-Id\": \"Root=1-65043b46-31bc2efb2ba67202432972da\",\n",
    "               \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "               \"Accept-Encoding\": \"gzip, deflate\",\n",
    "               \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "               \"Upgrade-Insecure-Requests\": \"1\"\n",
    "              }\n",
    "\n",
    "    try:\n",
    "        async with session.get(url, headers=headers) as response:\n",
    "            response.raise_for_status()\n",
    "            return await response.read()\n",
    "    except aiohttp.ClientError as e:\n",
    "        # Return empty byte to indicate error\n",
    "        print(f\"HTTP error occured for URL {url}: {e}\")\n",
    "        return b''\n",
    "\n",
    "        \n",
    "    \n",
    "# Gets data by fetching and scraping the pages asynchronously\n",
    "async def get_data(session: aiohttp.ClientSession, urls: List[str]) -> List:\n",
    "    all_data = []\n",
    "    tasks = [fetch_page(session, url) for url in urls]\n",
    "    pages = await asyncio.gather(*tasks)\n",
    "    \n",
    "    for page in pages:\n",
    "        data = scrape_page(page)\n",
    "        all_data.extend(data)\n",
    "        \n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1290463c-7949-487e-94a8-026ef5a2336d",
   "metadata": {},
   "source": [
    "### Main Execution\n",
    "The code following is main execution. It is what will have defined as a `main()`function, if we were to write this notebook into a script. It performs the following steps:\n",
    "- Initializes an empty list called `books` to store all the book data.\n",
    "- Creates a list of URLs (`urls`) representing the pages to be scraped (in this case, 50 pages).\n",
    "- Creates an asyncio event loop and a session using `aiohttp.ClientSession()`.\n",
    "- Uses `await get_data(session, urls)` to asynchronously fetch and scrape all the pages, and the collected data is stored in the `books_data` variable.\n",
    "- Finally, it appends the `books_data` to the `books` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f13f218c-cb14-45d2-a01e-64dc16fb1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "books: List[Any] = []\n",
    "BASE_URL: str = \"http://books.toscrape.com/catalogue/page-{}.html\"\n",
    "urls: List = [BASE_URL.format(i) for i in range(1,51)]\n",
    "    \n",
    "async with aiohttp.ClientSession() as session:\n",
    "    books_data = await get_data(session, urls)\n",
    "    \n",
    "    for page_data in books_data:\n",
    "        books.append(page_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d28337-b98d-4cc5-b693-b7194349c3a0",
   "metadata": {},
   "source": [
    "### Data Processing and Exporting\n",
    "In this cell, we perform the following actions:\n",
    "- Create a pandas DataFrame (`df`) from the `books` list, specifying column names ('Title', 'Price', 'Star Rating').\n",
    "- Save this DataFrame to a CSV file named \"books.csv\" using `df.to_csv()`.\n",
    "- Print \"Done!\" to indicate the completion of the scraping and exporting process.\n",
    "\n",
    "This cell is responsible for processing the collected data and storing it in a structured CSV format for further analysis or use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59637106-04a2-46c1-895d-0cfef1dc82ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(books, columns=['Title','Price', 'Star Rating'])\n",
    "df.to_csv('books.csv')\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data311",
   "language": "python",
   "name": "data311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
